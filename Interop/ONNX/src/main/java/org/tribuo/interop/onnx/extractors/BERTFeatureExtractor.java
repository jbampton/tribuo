/*
 * Copyright (c) 2021, 2024, Oracle and/or its affiliates. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.tribuo.interop.onnx.extractors;

import ai.onnxruntime.OnnxTensor;
import ai.onnxruntime.OnnxValue;
import ai.onnxruntime.OrtException;
import ai.onnxruntime.OrtSession;
import com.oracle.labs.mlrg.olcut.config.Config;
import com.oracle.labs.mlrg.olcut.config.PropertyException;
import com.oracle.labs.mlrg.olcut.provenance.ConfiguredObjectProvenance;
import com.oracle.labs.mlrg.olcut.provenance.impl.ConfiguredObjectProvenanceImpl;
import org.tribuo.Example;
import org.tribuo.Feature;
import org.tribuo.Output;
import org.tribuo.OutputFactory;
import org.tribuo.data.text.TextFeatureExtractor;
import org.tribuo.data.text.TextPipeline;
import org.tribuo.impl.ArrayExample;
import org.tribuo.sequence.SequenceExample;
import org.tribuo.util.embeddings.BERTTokenizerConfig;
import org.tribuo.util.embeddings.OnnxFeatureExtractor;
import org.tribuo.util.embeddings.processors.BERTInputProcessor;
import org.tribuo.util.embeddings.processors.BERTOutputProcessor;

import java.nio.BufferUnderflowException;
import java.nio.FloatBuffer;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.logging.Logger;

/**
 * Builds examples and sequence examples using features from BERT.
 * <p>
 * Assumes that the BERT is an ONNX model generated by HuggingFace Transformers and
 * exported using their export tool.
 * <p>
 * The tokenizer is expected to be a HuggingFace Transformers tokenizer config json file.
 * @see OnnxFeatureExtractor
 * @param <T> The output type.
 */
public class BERTFeatureExtractor<T extends Output<T>> implements AutoCloseable, TextFeatureExtractor<T>, TextPipeline {
    private static final Logger logger = Logger.getLogger(BERTFeatureExtractor.class.getName());

    /**
     * The type of output pooling to perform.
     */
    public enum OutputPooling {
        /**
         * Returns the CLS embedding.
         */
        CLS,
        /**
         * Takes the average of all the token embeddings
         */
        MEAN,
        /**
         * Uses the output of the BERT pooler, typically a transformed CLS embedding.
         */
        POOLER;
    }

    // BERT output names
    /**
     * Output name for the token level outputs.
     */
    public static final String TOKEN_OUTPUT = "output_0";

    // Metadata name for the token
    /**
     * Metadata key for the token value stored in a Tribuo {@link Example}.
     */
    public static final String TOKEN_METADATA = "Token";

    @Config(mandatory = true,description="Output factory to use.")
    private OutputFactory<T> outputFactory;

    @Config(mandatory=true,description="Path to the BERT model in ONNX format")
    private Path modelPath;

    @Config(mandatory=true,description="Path to the tokenizer config")
    private Path tokenizerPath;

    @Config(mandatory=true, description="Size of the embedding dimension.")
    private int embeddingDim;

    @Config(description="Maximum length in wordpieces")
    private int maxLength = 512;

    @Config(description="Type of pooling to use when returning a single embedding for the input sequence")
    private OutputPooling pooling = OutputPooling.CLS;

    @Config(description = "Use CUDA")
    private boolean useCUDA = false;

    // Cached feature names
    private String[] featureNames;

    private OnnxFeatureExtractor extractor;
    private boolean closed = false;

    /**
     * For OLCUT
     */
    private BERTFeatureExtractor() { }

    /**
     * Constructs a BERTFeatureExtractor.
     * @param outputFactory The output factory to use for building any unknown outputs.
     * @param modelPath The path to BERT in onnx format.
     * @param tokenizerPath The path to a Huggingface tokenizer json file.
     */
    public BERTFeatureExtractor(OutputFactory<T> outputFactory, Path modelPath, Path tokenizerPath, int embeddingDim) {
        this.outputFactory = outputFactory;
        this.modelPath = modelPath;
        this.tokenizerPath = tokenizerPath;
        this.embeddingDim = embeddingDim;
        postConfig();
    }

    /**
     * Constructs a BERTFeatureExtractor.
     * @param outputFactory The output factory to use for building any unknown outputs.
     * @param modelPath The path to BERT in onnx format.
     * @param tokenizerPath The path to a Huggingface tokenizer json file.
     * @param pooling The pooling type for extracted Examples.
     * @param maxLength The maximum number of wordpieces.
     * @param useCUDA Set to true to enable CUDA.
     */
    public BERTFeatureExtractor(OutputFactory<T> outputFactory, Path modelPath, Path tokenizerPath,
                                int embeddingDim, OutputPooling pooling, int maxLength, boolean useCUDA) {
        this.outputFactory = outputFactory;
        this.modelPath = modelPath;
        this.tokenizerPath = tokenizerPath;
        this.embeddingDim = embeddingDim;
        this.pooling = pooling;
        this.maxLength = maxLength;
        this.useCUDA = useCUDA;
        postConfig();
    }

    @Override
    public void postConfig() throws PropertyException {
        BERTInputProcessor inputProcessor = new BERTInputProcessor(tokenizerPath, maxLength, BERTTokenizerConfig.class);
        BERTOutputProcessor.BERTPooling bertPooling = switch(pooling) {
            case CLS -> BERTOutputProcessor.BERTPooling.CLS;
            case MEAN -> BERTOutputProcessor.BERTPooling.MEAN;
            case POOLER -> BERTOutputProcessor.BERTPooling.POOLER;
        };
        BERTOutputProcessor outputProcessor = new BERTOutputProcessor(bertPooling, embeddingDim, false, true, TOKEN_OUTPUT);
        extractor = new OnnxFeatureExtractor(modelPath, inputProcessor, outputProcessor, useCUDA, 1, 1);
        featureNames = generateFeatureNames(embeddingDim);
    }

    @Override
    public ConfiguredObjectProvenance getProvenance() {
        return new ConfiguredObjectProvenanceImpl(this,"FeatureExtractor");
    }

    /**
     * Reconstructs the OrtSession using the supplied options.
     * This allows the use of different computation backends and
     * configurations.
     * @param options The new session options.
     * @throws OrtException If the native runtime failed to rebuild itself.
     */
    public void reconfigureOrtSession(OrtSession.SessionOptions options) throws OrtException {
        extractor.reconfigureOrtSession(options);
    }

    /**
     * Returns the maximum length this BERT will accept.
     * @return The maximum number of tokens (including [CLS] and [SEP], so the maximum is effectively 2 less than this).
     */
    public int getMaxLength() {
        return maxLength;
    }

    /**
     * Returns the vocabulary that this BERTFeatureExtractor understands.
     * @return The vocabulary.
     */
    public Set<String> getVocab() {
        return extractor.getVocab();
    }

    /**
     * Generates the feature names in the range 0 to {@code bertDim}.
     * <p>
     * Feature names are of the form "D=id".
     * @param bertDim The number of dimensions in this BERT's representation.
     * @return The feature names;
     */
    private static String[] generateFeatureNames(int bertDim) {
        int width = (""+bertDim).length();
        String formatString = "D=%0"+width+"d";

        String[] names = new String[bertDim];
        for (int i = 0; i < bertDim; i++) {
            names[i] = String.format(formatString,i);
        }

        return names;
    }

    /**
     * Reads bertDim values off the buffer, throwing {@link BufferUnderflowException} if we exceed the buffer.
     * <p>
     * Advances the state of the buffer.
     * @param buffer The float buffer to read.
     * @param bertDim The number of values to read.
     * @return The features.
     */
    private static double[] extractFeatures(FloatBuffer buffer, int bertDim) {
        double[] features = new double[bertDim];
        float[] floatArr = new float[bertDim];
        buffer.get(floatArr);
        for (int i = 0; i < floatArr.length; i++) {
            features[i] = floatArr[i];
        }
        return features;
    }

    /**
     * Passes the tokens through BERT, replacing any unknown tokens with the [UNK] token.
     * <p>
     * The features of the returned example are dense, the output is the output factory's unknown output.
     * <p>
     * Throws {@link IllegalArgumentException} if the list is longer than {@link #getMaxLength}.
     * Throws {@link IllegalStateException} if the BERT model failed to produce an output.
     * @param input The input text.
     * @return A dense example representing the pooled output from BERT for the input text.
     */
    public Example<T> extract(String input) {
        return extract(outputFactory.getUnknownOutput(), input);
    }

    /**
     * Passes the tokens through BERT, replacing any unknown tokens with the [UNK] token.
     * <p>
     * The features of each example are dense.
     * If {@code stripSentenceMarkers} is true then the [CLS] and [SEP] tokens are removed before example generation.
     * If it's false then they are left in with the appropriate unknown output set.
     * <p>
     * Throws {@link IllegalArgumentException} if the list is longer than {@link #getMaxLength}.
     * Throws {@link IllegalStateException} if the BERT model failed to produce an output.
     * @param tokens The input tokens. Should be tokenized using the Tokenizer this BERT expects.
     * @param stripSentenceMarkers Remove the [CLS] and [SEP] tokens from the returned example.
     * @return A dense sequence example representing the token level output from BERT.
     */
    public SequenceExample<T> extractSequenceExample(List<String> tokens, boolean stripSentenceMarkers) {
        List<T> outputs = new ArrayList<>();
        for (int i = 0; i < tokens.size(); i++) {
            outputs.add(outputFactory.getUnknownOutput());
        }
        return extractSequenceExample(tokens,outputs,stripSentenceMarkers);
    }

    /**
     * Passes the tokens through BERT, replacing any unknown tokens with the [UNK] token.
     * <p>
     * The features of each example are dense. The output list must be the same length as the number of tokens.
     * If {@code stripSentenceMarkers} is true then the [CLS] and [SEP] tokens are removed before example generation.
     * If it's false then they are left in with the appropriate unknown output set.
     * <p>
     * Throws {@link IllegalArgumentException} if the list is longer than {@link #getMaxLength}.
     * Throws {@link IllegalStateException} if the BERT model failed to produce an output.
     * @param tokens The input tokens. Should be tokenized using the Tokenizer this BERT expects.
     * @param output The ground truth output for this example.
     * @param stripSentenceMarkers Remove the [CLS] and [SEP] tokens from the returned example.
     * @return A dense sequence example representing the token level output from BERT.
     */
    public SequenceExample<T> extractSequenceExample(List<String> tokens, List<T> output, boolean stripSentenceMarkers) {
        if (tokens.size() > (maxLength - 2)) {
            throw new IllegalArgumentException("Too many tokens, expected " + (maxLength - 2) + " found " + tokens.size());
        }
        try (OnnxTensor tokenIds = convertTokens(tokens);
             OnnxTensor mask = createTensor(tokens.size()+2,MASK_VALUE);
             OnnxTensor tokenTypes = createTensor(tokens.size()+2,TOKEN_TYPE_VALUE)) {
            Map<String,OnnxTensor> inputMap = new HashMap<>(3);
            inputMap.put(INPUT_IDS,tokenIds);
            inputMap.put(ATTENTION_MASK,mask);
            inputMap.put(TOKEN_TYPE_IDS,tokenTypes);
            try (OrtSession.Result bertOutput = session.run(inputMap)) {
                OnnxValue value = bertOutput.get(TOKEN_OUTPUT).orElseThrow(() -> new IllegalStateException("Failed to read " + TOKEN_OUTPUT + " from the BERT response"));
                if (value instanceof OnnxTensor) {
                    OnnxTensor tensor = (OnnxTensor) value;
                    FloatBuffer buffer = tensor.getFloatBuffer();
                    if (buffer != null) {
                        List<Example<T>> examples = new ArrayList<>();

                        // Add the [CLS] token if necessary
                        if (stripSentenceMarkers) {
                            // throw away the features
                            buffer.position(embeddingDim);
                        } else {
                            double[] featureValues = extractFeatures(buffer, embeddingDim);
                            Example<T> tmp = new ArrayExample<>(outputFactory.getUnknownOutput(),featureNames,featureValues);
                            tmp.setMetadataValue(TOKEN_METADATA,CLASSIFICATION_TOKEN);
                            examples.add(tmp);
                        }

                        // iterate the tokens, creating new examples
                        for (int i = 0; i < tokens.size(); i++) {
                            double[] featureValues = extractFeatures(buffer, embeddingDim);
                            Example<T> tmp = new ArrayExample<T>(output.get(i),featureNames,featureValues);
                            tmp.setMetadataValue(TOKEN_METADATA,tokens.get(i));
                            examples.add(tmp);
                        }

                        // Add the [SEP] token if necessary
                        if (!stripSentenceMarkers) {
                            double[] featureValues = extractFeatures(buffer, embeddingDim);
                            Example<T> tmp = new ArrayExample<>(outputFactory.getUnknownOutput(),featureNames,featureValues);
                            tmp.setMetadataValue(TOKEN_METADATA,SEPARATOR_TOKEN);
                            examples.add(tmp);
                        }

                        return new SequenceExample<>(examples);
                    } else {
                        throw new IllegalStateException("Expected a float tensor, found " + tensor.getInfo().toString());
                    }
                } else {
                    throw new IllegalStateException("Expected OnnxTensor, found " + value.getClass());
                }
            }
        } catch (OrtException e) {
            throw new IllegalStateException("ORT failed to execute: ", e);
        }
    }

    @Override
    public void close() throws OrtException {
        if (!closed) {
            extractor.close();
            closed = true;
        }
    }

    /**
     * Tokenizes the input using the loaded tokenizer, truncates the
     * token list if it's longer than {@code maxLength} - 2 (to account
     * for [CLS] and [SEP] tokens), and then passes the token
     * list to {@link #extractExample}.
     * @param output The output object.
     * @param data The input text.
     * @return An example containing BERT embedding features and the requested output.
     */
    @Override
    public Example<T> extract(T output, String data) {
        List<String> tokens = tokenize(data);
        return extractExample(tokens,output);
    }

    /**
     * Tokenizes the input using the loaded tokenizer, truncates the
     * token list if it's longer than {@code maxLength} - 2 (to account
     * for [CLS] and [SEP] tokens), and then passes the token
     * list to {@link #extractExample}.
     * @param tag A tag to prefix all the generated feature names with.
     * @param data The input text.
     * @return The BERT features for the supplied data.
     */
    @Override
    public List<Feature> process(String tag, String data) {
        List<String> tokens = tokenize(data);
        double[] featureValues = extractFeatures(tokens);
        List<Feature> features = new ArrayList<>(featureValues.length);
        for (int i = 0; i < featureValues.length; i++) {
            features.add(new Feature(tag + "-" + featureNames[i],featureValues[i]));
        }
        return features;
    }

}
